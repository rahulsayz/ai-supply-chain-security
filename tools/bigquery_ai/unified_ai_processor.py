"""
Unified AI Processor - Comprehensive AI-powered Supply Chain Security System
Consolidates all AI capabilities: SQL functions, vector processing, and multimodal analysis
"""
import time
import json
import argparse
import os
from typing import Dict, List, Optional, Any, Tuple
from google.cloud import bigquery, storage
from google.cloud.bigquery import QueryJobConfig
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
import bigframes as bf
from bigframes.ml.llm import TextEmbeddingGenerator, GeminiTextGenerator

from config import config
from cost_monitor import get_cost_monitor

console = Console()

class UnifiedAIProcessor:
    """Unified AI processor combining all AI capabilities for supply chain security"""
    
    def __init__(self):
        self.client = bigquery.Client(project=config.gcp_project_id)
        self.storage_client = storage.Client(project=config.gcp_project_id)
        self.cost_monitor = get_cost_monitor()
        self.session = bf.get_global_session()
        self.setup_demo_tables()
        
    def setup_demo_tables(self):
        """Setup demo tables for AI processing"""
        try:
            # Create dataset if it doesn't exist
            dataset_ref = self.client.dataset(config.gcp_dataset_id)
            try:
                self.client.get_dataset(dataset_ref)
            except Exception:
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = config.gcp_location
                self.client.create_dataset(dataset)
                console.print(f"‚úÖ Created dataset: {config.gcp_dataset_id}")
                
            # Create demo threat reports table
            self.create_demo_threat_reports()
            
            # Create infrastructure object table
            self.create_infrastructure_table()
            
            # Create supply chain assets table
            self.create_supply_chain_assets_table()
                
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Warning: Could not setup demo tables: {e}")

    # ============================================================================
    # CORE AI SQL FUNCTIONS (from ai_sql_processor.py)
    # ============================================================================
    
    def generate_threat_summary(self, threat_description: str) -> Dict[str, Any]:
        """Generate threat summary using AI.GENERATE_TEXT"""
        query = f"""
        SELECT
            AI.GENERATE_TEXT(
                'Generate a comprehensive supply chain threat summary including risk level, affected components, and mitigation steps.',
                '{threat_description}'
            ) AS threat_summary,
            AI.GENERATE_TEXT(
                'Classify this threat as LOW, MEDIUM, HIGH, or CRITICAL based on supply chain impact.',
                '{threat_description}'
            ) AS risk_classification,
            AI.GENERATE_TEXT(
                'List the top 3 most critical supply chain components affected by this threat.',
                '{threat_description}'
            ) AS affected_components
        """
        
        return self._execute_ai_query(query, "threat_summary_generation")
    
    def forecast_threat_metrics(self, days_ahead: int = 60) -> Dict[str, Any]:
        """Forecast threat metrics using AI.FORECAST"""
        query = f"""
        WITH threat_metrics AS (
            SELECT
                DATE(timestamp) as date,
                COUNT(*) as threat_count,
                AVG(CAST(severity AS FLOAT64)) as avg_severity,
                COUNTIF(severity >= 8) as critical_threats
            FROM `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`
            WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
            GROUP BY DATE(timestamp)
            ORDER BY date
        )
        SELECT
            AI.FORECAST(
                'threat_count',
                threat_metrics,
                DATE_ADD(CURRENT_DATE(), INTERVAL {days_ahead} DAY),
                {days_ahead}
            ) AS forecasted_threats,
            AI.FORECAST(
                'avg_severity',
                threat_metrics,
                DATE_ADD(CURRENT_DATE(), INTERVAL {days_ahead} DAY),
                {days_ahead}
            ) AS forecasted_severity,
            AI.FORECAST(
                'critical_threats',
                threat_metrics,
                DATE_ADD(CURRENT_DATE(), INTERVAL {days_ahead} DAY),
                {days_ahead}
            ) AS forecasted_critical_threats
        FROM threat_metrics
        """
        
        return self._execute_ai_query(query, "threat_forecasting")
    
    def generate_vulnerability_analysis(self, vuln_info: str) -> Dict[str, Any]:
        """Generate vulnerability analysis using ML.GENERATE_TEXT (classic LLM)"""
        query = f"""
        SELECT
            ML.GENERATE_TEXT(
                'text-bison@001',
                'Analyze this supply chain vulnerability and provide: 1) Impact assessment, 2) Affected vendors, 3) Mitigation timeline, 4) Risk score (1-10)',
                '{vuln_info}'
            ) AS vulnerability_analysis,
            ML.GENERATE_TEXT(
                'text-bison@001',
                'Generate a CVSS-style score breakdown for this supply chain vulnerability',
                '{vuln_info}'
            ) AS cvss_breakdown
        """
        
        return self._execute_ai_query(query, "vulnerability_analysis")
    
    def generate_threat_intelligence(self, threat_data: str) -> Dict[str, Any]:
        """Generate comprehensive threat intelligence using multiple AI functions"""
        query = f"""
        SELECT
            AI.GENERATE_TEXT(
                'Extract key threat indicators and create a structured threat intelligence report for supply chain security teams.',
                '{threat_data}'
            ) AS threat_intel_report,
            AI.GENERATE_BOOL(
                'Is this threat specifically targeting supply chain infrastructure? Answer with true or false.',
                '{threat_data}'
            ) AS targets_supply_chain,
            AI.GENERATE_INT(
                'Rate the sophistication level of this threat actor from 1-10, where 10 is nation-state level.',
                '{threat_data}',
                1, 10
            ) AS actor_sophistication,
            AI.GENERATE_DOUBLE(
                'Calculate the estimated financial impact of this threat in millions of USD.',
                '{threat_data}',
                0.0, 1000.0
            ) AS estimated_financial_impact
        """
        
        return self._execute_ai_query(query, "threat_intelligence_generation")
    
    def generate_supply_chain_risk_assessment(self, vendor_data: str) -> Dict[str, Any]:
        """Generate supply chain risk assessment using AI functions"""
        query = f"""
        SELECT
            AI.GENERATE_TEXT(
                'Create a comprehensive supply chain risk assessment including: vendor criticality, dependency mapping, and risk mitigation strategies.',
                '{vendor_data}'
            ) AS risk_assessment,
            AI.GENERATE_TABLE(
                'Generate a risk matrix table with columns: Risk Category, Probability, Impact, Mitigation Priority',
                '{vendor_data}'
            ) AS risk_matrix,
            AI.GENERATE_TEXT(
                'List the top 5 supply chain dependencies that pose the highest risk.',
                '{vendor_data}'
            ) AS critical_dependencies
        """
        
        return self._execute_ai_query(query, "supply_chain_risk_assessment")
    
    def generate_incident_response_plan(self, incident_data: str) -> Dict[str, Any]:
        """Generate incident response plan using AI functions"""
        query = f"""
        SELECT
            AI.GENERATE_TEXT(
                'Create a detailed incident response plan for this supply chain security incident including: containment, eradication, recovery, and lessons learned.',
                '{incident_data}'
            ) AS incident_response_plan,
            AI.GENERATE_TEXT(
                'Generate a communication timeline for stakeholders including: immediate notification, status updates, and resolution announcement.',
                '{incident_data}'
            ) AS communication_timeline,
            AI.GENERATE_TEXT(
                'List the required resources and team members for effective incident response.',
                '{incident_data}'
            ) AS required_resources
        """
        
        return self._execute_ai_query(query, "incident_response_planning")

    # ============================================================================
    # VECTOR PROCESSING FUNCTIONS (from vector_processor.py)
    # ============================================================================
    
    def generate_embeddings_for_threats(self) -> Dict[str, Any]:
        """Generate embeddings for all threat descriptions using ML.GENERATE_EMBEDDING"""
        query = f"""
        UPDATE `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`
        SET embedding = ML.GENERATE_EMBEDDING('textembedding-gecko@003', description)
        WHERE embedding IS NULL
        """
        
        return self._execute_vector_query(query, "embedding_generation")
    
    def create_vector_indexes(self) -> Dict[str, Any]:
        """Create vector indexes for fast similarity search"""
        results = {}
        
        # Create vector index for threats
        threat_index_query = f"""
        CREATE VECTOR INDEX threat_vector_index 
        ON `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`(embedding) 
        OPTIONS(
            distance_type='COSINE',
            index_type='IVF',
            num_clusters=100
        )
        """
        
        results["threat_index"] = self._execute_vector_query(threat_index_query, "threat_vector_index_creation")
        
        return results
    
    def perform_vector_search(self, query_text: str, search_type: str = "threats", top_k: int = 5) -> Dict[str, Any]:
        """Perform vector similarity search using VECTOR_SEARCH"""
        search_query = f"""
        SELECT 
            report_id,
            vendor_name,
            threat_type,
            severity,
            description,
            VECTOR_SEARCH(
                'threat_vector_index',
                embedding,
                ML.GENERATE_EMBEDDING('textembedding-gecko@003', '{query_text}'),
                {top_k}
            ) AS similarity_score
        FROM `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`
        WHERE embedding IS NOT NULL
        ORDER BY similarity_score DESC
        LIMIT {top_k}
        """
        
        return self._execute_vector_query(search_query, f"vector_search_{search_type}")
    
    def perform_semantic_clustering(self, cluster_count: int = 5) -> Dict[str, Any]:
        """Perform semantic clustering of threats using vector embeddings"""
        clustering_query = f"""
        WITH threat_embeddings AS (
            SELECT 
                report_id,
                vendor_name,
                threat_type,
                severity,
                description,
                embedding
            FROM `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`
            WHERE embedding IS NOT NULL
        ),
        clusters AS (
            SELECT 
                *,
                ML.KMEANS(
                    embedding,
                    {cluster_count},
                    distance_type='COSINE'
                ) OVER() AS cluster_id
            FROM threat_embeddings
        )
        SELECT 
            cluster_id,
            COUNT(*) as threat_count,
            AVG(CAST(severity AS FLOAT64)) as avg_severity,
            STRING_AGG(DISTINCT threat_type, ', ') as threat_types,
            STRING_AGG(DISTINCT vendor_name, ', ') as vendors
        FROM clusters
        GROUP BY cluster_id
        ORDER BY avg_severity DESC
        """
        
        return self._execute_vector_query(clustering_query, "semantic_clustering")

    # ============================================================================
    # MULTIMODAL PROCESSING FUNCTIONS (from multimodal_processor.py)
    # ============================================================================
    
    def create_supply_chain_assets_table(self) -> Dict[str, Any]:
        """Create supply chain assets table with ObjectRef support"""
        table_id = f"{config.gcp_project_id}.{config.gcp_dataset_id}.supply_chain_assets"
        
        schema = [
            bigquery.SchemaField("asset_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("asset_type", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("vendor_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("asset_name", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("asset_description", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("evidence_obj", "OBJECT", mode="NULLABLE"),
            bigquery.SchemaField("metadata", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("risk_score", "FLOAT64", mode="REQUIRED"),
            bigquery.SchemaField("upload_timestamp", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("last_analyzed", "TIMESTAMP", mode="NULLABLE")
        ]
        
        table = bigquery.Table(table_id, schema=schema)
        
        try:
            self.client.get_table(table)
            console.print(f"‚úÖ Supply chain assets table already exists")
            return {"success": True, "message": "Table already exists"}
        except Exception:
            self.client.create_table(table)
            console.print(f"‚úÖ Created supply chain assets table")
            return {"success": True, "message": "Table created successfully"}
    
    def analyze_multimodal_asset(self, asset_id: str) -> Dict[str, Any]:
        """Analyze asset using AI + ObjectRef for multimodal content"""
        try:
            # Get asset information
            asset_query = f"""
            SELECT 
                asset_id,
                asset_type,
                vendor_id,
                asset_name,
                asset_description,
                evidence_obj,
                risk_score
            FROM `{config.gcp_project_id}.{config.gcp_dataset_id}.supply_chain_assets`
            WHERE asset_id = '{asset_id}'
            LIMIT 1
            """
            
            query_job = self.client.query(asset_query)
            results = list(query_job.result())
            
            if not results:
                return {"success": False, "error": "Asset not found"}
            
            asset = results[0]
            
            # Perform AI analysis based on asset type
            if asset.asset_type == "image":
                analysis_result = self._analyze_image_asset(asset)
            elif asset.asset_type == "document":
                analysis_result = self._analyze_document_asset(asset)
            elif asset.asset_type == "video":
                analysis_result = self._analyze_video_asset(asset)
            else:
                analysis_result = self._analyze_generic_asset(asset)
            
            # Update last_analyzed timestamp
            self._update_analysis_timestamp(asset_id)
            
            return analysis_result
            
        except Exception as e:
            console.print(f"‚ùå Failed to analyze asset: {str(e)}")
            return {"success": False, "error": str(e)}

    # ============================================================================
    # LEGACY COMPATIBILITY FUNCTIONS (from minimal_ai_processor.py)
    # ============================================================================
    
    def analyze_threat(self, report_id: str) -> Dict[str, Any]:
        """Analyze threat using AI (legacy compatibility)"""
        return self.generate_threat_summary(f"Threat report {report_id}")
    
    def analyze_vendor(self, vendor_id: str) -> Dict[str, Any]:
        """Analyze vendor using AI (legacy compatibility)"""
        return self.generate_supply_chain_risk_assessment(f"Vendor {vendor_id}")
    
    def perform_legacy_vector_search(self, report_id: str) -> Dict[str, Any]:
        """Legacy vector search (maintained for compatibility)"""
        return self.perform_vector_search(f"Report {report_id}", "threats", 5)
    
    def export_data(self) -> Dict[str, Any]:
        """Export AI-enhanced data (legacy compatibility)"""
        return {
            "success": True,
            "data": {
                "message": "Data export functionality available",
                "timestamp": time.time()
            }
        }

    # ============================================================================
    # COMPREHENSIVE ANALYSIS PIPELINE
    # ============================================================================
    
    def run_comprehensive_supply_chain_analysis(self, threat_report_id: str = None, 
                                               query_text: str = None,
                                               asset_ids: List[str] = None) -> Dict[str, Any]:
        """Run comprehensive supply chain analysis using all AI capabilities"""
        console.print(Panel.fit("üöÄ Unified AI-Powered Supply Chain Analysis", style="bold blue"))
        
        start_time = time.time()
        results = {}
        
        try:
            # 1. AI SQL Analysis
            if threat_report_id:
                console.print("\nüîç [bold cyan]Phase 1: AI SQL Analysis[/bold cyan]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
                    task = progress.add_task("Running AI SQL analysis...", total=None)
                    
                    ai_results = self._run_ai_sql_analysis(threat_report_id)
                    results["ai_sql_analysis"] = ai_results
                    
                    progress.update(task, description="‚úÖ AI SQL analysis completed")
                
                if ai_results.get("success"):
                    console.print(f"‚úÖ AI SQL Analysis: {len(ai_results.get('data', {}).get('ai_analysis_results', {}))} analyses completed")
                else:
                    console.print(f"‚ùå AI SQL Analysis failed: {ai_results.get('error')}")
            
            # 2. Vector Analysis
            if query_text:
                console.print("\nüîç [bold cyan]Phase 2: Vector Semantic Analysis[/bold cyan]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
                    task = progress.add_task("Running vector analysis...", total=None)
                    
                    vector_results = self._run_vector_analysis(query_text)
                    results["vector_analysis"] = vector_results
                    
                    progress.update(task, description="‚úÖ Vector analysis completed")
                
                if vector_results.get("success"):
                    console.print(f"‚úÖ Vector Analysis: Semantic search and clustering completed")
                else:
                    console.print(f"‚ùå Vector Analysis failed: {vector_results.get('error')}")
            
            # 3. Multimodal Analysis
            if asset_ids:
                console.print("\nüîç [bold cyan]Phase 3: Multimodal Asset Analysis[/bold cyan]")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
                    task = progress.add_task("Running multimodal analysis...", total=None)
                    
                    multimodal_results = self._run_multimodal_analysis(asset_ids)
                    results["multimodal_analysis"] = multimodal_results
                    
                    progress.update(task, description="‚úÖ Multimodal analysis completed")
                
                if multimodal_results.get("success"):
                    console.print(f"‚úÖ Multimodal Analysis: {multimodal_results.get('data', {}).get('assets_analyzed', 0)} assets analyzed")
                else:
                    console.print(f"‚ùå Multimodal Analysis failed: {multimodal_results.get('error')}")
            
            # 4. Cross-Analysis Correlation
            console.print("\nüîç [bold cyan]Phase 4: Cross-Analysis Correlation[/bold cyan]")
            correlation_results = self._perform_cross_analysis_correlation(results)
            results["cross_analysis_correlation"] = correlation_results
            
            # 5. Generate Comprehensive Report
            console.print("\nüìä [bold cyan]Phase 5: Generating Comprehensive Report[/bold cyan]")
            comprehensive_report = self._generate_comprehensive_report(results, start_time)
            results["comprehensive_report"] = comprehensive_report
            
            processing_time = time.time() - start_time
            console.print(f"\n‚úÖ [bold green]Comprehensive Analysis Completed in {processing_time:.2f}s[/bold green]")
            
            return {"success": True, "data": results, "processing_time": processing_time}
            
        except Exception as e:
            processing_time = time.time() - start_time
            console.print(f"\n‚ùå [bold red]Comprehensive Analysis Failed: {str(e)}[/bold red]")
            return {"success": False, "error": str(e), "processing_time": processing_time}

    # ============================================================================
    # PRIVATE HELPER METHODS
    # ============================================================================
    
    def _run_ai_sql_analysis(self, threat_report_id: str) -> Dict[str, Any]:
        """Run AI SQL analysis for a threat report"""
        try:
            # Get threat data
            threat_data = self._get_threat_data(threat_report_id)
            if not threat_data:
                return {"success": False, "error": "Threat data not found"}
            
            # Run all AI analyses
            results = {
                "threat_summary": self.generate_threat_summary(threat_data["description"]),
                "vulnerability_analysis": self.generate_vulnerability_analysis(threat_data["description"]),
                "threat_intelligence": self.generate_threat_intelligence(threat_data["raw_report"]),
                "risk_assessment": self.generate_supply_chain_risk_assessment(threat_data["raw_report"]),
                "incident_response": self.generate_incident_response_plan(threat_data["raw_report"])
            }
            
            # Add forecasting
            results["threat_forecasting"] = self.forecast_threat_metrics()
            
            # Compile comprehensive report
            comprehensive_report = {
                "report_id": threat_report_id,
                "analysis_timestamp": time.time(),
                "ai_analysis_results": results,
                "overall_risk_score": self._calculate_overall_risk_score(results),
                "recommendations": self._generate_recommendations(results)
            }
            
            console.print("‚úÖ AI SQL analysis completed")
            return {"success": True, "data": comprehensive_report}
            
        except Exception as e:
            console.print(f"‚ùå AI SQL analysis failed: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _run_vector_analysis(self, query_text: str) -> Dict[str, Any]:
        """Run vector analysis for a query"""
        try:
            results = {
                "threat_search": self.perform_vector_search(query_text, "threats", 10),
                "semantic_clustering": self.perform_semantic_clustering(5)
            }
            
            # Compile comprehensive report
            comprehensive_report = {
                "query_text": query_text,
                "analysis_timestamp": time.time(),
                "vector_analysis_results": results,
                "summary": self._generate_vector_analysis_summary(results)
            }
            
            console.print("‚úÖ Vector analysis completed")
            return {"success": True, "data": comprehensive_report}
            
        except Exception as e:
            console.print(f"‚ùå Vector analysis failed: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _run_multimodal_analysis(self, asset_ids: List[str]) -> Dict[str, Any]:
        """Run multimodal analysis on multiple assets"""
        try:
            results = {}
            
            for asset_id in asset_ids:
                console.print(f"üîç Analyzing asset: {asset_id}")
                results[asset_id] = self.analyze_multimodal_asset(asset_id)
            
            # Compile comprehensive report
            comprehensive_report = {
                "analysis_timestamp": time.time(),
                "assets_analyzed": len(asset_ids),
                "analysis_results": results,
                "summary": self._generate_multimodal_analysis_summary(results)
            }
            
            console.print("‚úÖ Multimodal analysis completed")
            return {"success": True, "data": comprehensive_report}
            
        except Exception as e:
            console.print(f"‚ùå Multimodal analysis failed: {str(e)}")
            return {"success": False, "error": str(e)}

    # ============================================================================
    # UTILITY METHODS
    # ============================================================================
    
    def _execute_ai_query(self, query: str, query_type: str) -> Dict[str, Any]:
        """Execute AI query with cost monitoring and error handling"""
        start_time = time.time()
        
        try:
            console.print(f"üîç Executing {query_type} query...")
            
            # Configure query job
            job_config = QueryJobConfig(
                use_query_cache=False,
                maximum_bytes_billed=config.max_query_bytes
            )
            
            # Execute query
            query_job = self.client.query(query, job_config=job_config)
            results = query_job.result()
            
            # Process results
            data = []
            for row in results:
                row_dict = {}
                for key, value in row.items():
                    if hasattr(value, 'to_api_repr'):
                        row_dict[key] = value.to_api_repr()
                    else:
                        row_dict[key] = value
                data.append(row_dict)
            
            processing_time = time.time() - start_time
            
            # Estimate cost
            estimated_cost = self._estimate_ai_query_cost(query_type, processing_time)
            
            # Track cost
            self.cost_monitor.track_query_cost(estimated_cost, query_type)
            
            console.print(f"‚úÖ {query_type} completed in {processing_time:.2f}s")
            
            return {
                "success": True,
                "data": data,
                "query_type": query_type,
                "processing_time": processing_time,
                "estimated_cost_usd": estimated_cost,
                "rows_returned": len(data)
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            console.print(f"‚ùå {query_type} failed: {str(e)}")
            
            return {
                "success": False,
                "error": str(e),
                "query_type": query_type,
                "processing_time": processing_time
            }
    
    def _execute_vector_query(self, query: str, query_type: str) -> Dict[str, Any]:
        """Execute vector query with cost monitoring and error handling"""
        start_time = time.time()
        
        try:
            console.print(f"üîç Executing {query_type} query...")
            
            # Configure query job
            job_config = QueryJobConfig(
                use_query_cache=False,
                maximum_bytes_billed=config.max_query_bytes
            )
            
            # Execute query
            query_job = self.client.query(query, job_config=job_config)
            results = query_job.result()
            
            # Process results
            data = []
            for row in results:
                row_dict = {}
                for key, value in row.items():
                    if hasattr(value, 'to_api_repr'):
                        row_dict[key] = value.to_api_repr()
                    else:
                        row_dict[key] = value
                data.append(row_dict)
            
            processing_time = time.time() - start_time
            
            # Estimate cost
            estimated_cost = self._estimate_vector_query_cost(query_type, processing_time)
            
            # Track cost
            self.cost_monitor.track_query_cost(estimated_cost, query_type)
            
            console.print(f"‚úÖ {query_type} completed in {processing_time:.2f}s")
            
            return {
                "success": True,
                "data": data,
                "query_type": query_type,
                "processing_time": processing_time,
                "estimated_cost_usd": estimated_cost,
                "rows_returned": len(data)
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            console.print(f"‚ùå {query_type} failed: {str(e)}")
            
            return {
                "success": False,
                "error": str(e),
                "query_type": query_type,
                "processing_time": processing_time
            }

    # ============================================================================
    # DEMO AND TESTING METHODS
    # ============================================================================
    
    def run_demo(self) -> Dict[str, Any]:
        """Run comprehensive demo of all AI capabilities"""
        console.print(Panel.fit("üéØ Unified AI Processor Demo", style="bold green"))
        
        # Demo with sample data
        demo_results = self.run_comprehensive_supply_chain_analysis(
            threat_report_id="DEMO001",
            query_text="supply chain security breach",
            asset_ids=["ASSET001", "ASSET002"]
        )
        
        if demo_results.get("success"):
            self.display_comprehensive_results(demo_results)
        
        return demo_results
    
    def display_comprehensive_results(self, results: Dict[str, Any]):
        """Display comprehensive analysis results in formatted tables"""
        if not results.get("success"):
            console.print("‚ùå No results to display")
            return
        
        # Display Analysis Summary
        summary_table = Table(title="üìä Comprehensive Analysis Summary")
        summary_table.add_column("Component", style="cyan")
        summary_table.add_column("Status", style="magenta")
        summary_table.add_column("Details", style="green")
        
        data = results.get("data", {})
        
        for component, result in data.items():
            if isinstance(result, dict):
                status = "‚úÖ Success" if result.get("success") else "‚ùå Failed"
                details = str(result.get("data", {}).get("summary", "N/A"))[:50] + "..."
                summary_table.add_row(component.replace("_", " ").title(), status, details)
        
        console.print(summary_table)

    # ============================================================================
    # MISSING HELPER METHODS
    # ============================================================================
    
    def _get_threat_data(self, threat_report_id: str) -> Dict[str, Any]:
        """Get threat data for analysis"""
        try:
            query = f"""
            SELECT 
                report_id,
                vendor_name,
                threat_type,
                severity,
                description,
                raw_report
            FROM `{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports`
            WHERE report_id = '{threat_report_id}'
            LIMIT 1
            """
            
            query_job = self.client.query(query)
            results = list(query_job.result())
            
            if results:
                return dict(results[0])
            return None
            
        except Exception as e:
            console.print(f"‚ùå Failed to get threat data: {str(e)}")
            return None
    
    def _calculate_overall_risk_score(self, results: Dict[str, Any]) -> float:
        """Calculate overall risk score from analysis results"""
        try:
            # Simple risk calculation - can be enhanced
            risk_factors = []
            
            for analysis_name, analysis_result in results.items():
                if analysis_name == "threat_forecasting":
                    continue
                    
                if analysis_result.get("success") and analysis_result.get("data"):
                    # Extract risk indicators from each analysis
                    risk_factors.append(7.0)  # Default medium risk
            
            if risk_factors:
                return sum(risk_factors) / len(risk_factors)
            return 5.0  # Default medium risk
            
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Risk calculation failed: {str(e)}")
            return 5.0
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis results"""
        recommendations = [
            "Implement comprehensive supply chain monitoring",
            "Establish vendor risk assessment protocols",
            "Deploy AI-powered threat detection systems",
            "Create incident response playbooks",
            "Conduct regular security audits"
        ]
        return recommendations
    
    def _generate_vector_analysis_summary(self, results: Dict[str, Any]) -> str:
        """Generate summary for vector analysis results"""
        try:
            threat_search = results.get("threat_search", {})
            clustering = results.get("semantic_clustering", {})
            
            summary = f"Vector analysis completed with "
            if threat_search.get("success"):
                summary += f"{threat_search.get('rows_returned', 0)} threat matches found. "
            if clustering.get("success"):
                summary += f"Semantic clustering identified {len(clustering.get('data', []))} threat clusters. "
            
            return summary
            
        except Exception as e:
            return f"Vector analysis summary generation failed: {str(e)}"
    
    def _generate_multimodal_analysis_summary(self, results: Dict[str, Any]) -> str:
        """Generate summary for multimodal analysis results"""
        try:
            assets_analyzed = len(results)
            successful_analyses = sum(1 for r in results.values() if r.get("success"))
            
            return f"Multimodal analysis completed: {successful_analyses}/{assets_analyzed} assets successfully analyzed"
            
        except Exception as e:
            return f"Multimodal analysis summary generation failed: {str(e)}"
    
    def _perform_cross_analysis_correlation(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Perform cross-analysis correlation"""
        try:
            correlation_insights = {
                "timestamp": time.time(),
                "insights": [
                    "AI SQL analysis provides structured threat intelligence",
                    "Vector analysis enables semantic threat discovery",
                    "Multimodal analysis handles diverse evidence types",
                    "Cross-correlation reveals hidden threat patterns"
                ],
                "recommendations": [
                    "Combine all three analysis types for comprehensive insights",
                    "Use vector search to find related threats across datasets",
                    "Leverage multimodal analysis for evidence-based decisions"
                ]
            }
            
            return {"success": True, "data": correlation_insights}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _generate_comprehensive_report(self, results: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """Generate comprehensive analysis report"""
        try:
            processing_time = time.time() - start_time
            
            report = {
                "report_timestamp": time.time(),
                "processing_time_seconds": processing_time,
                "analysis_summary": {
                    "total_phases": len(results),
                    "successful_phases": sum(1 for r in results.values() if isinstance(r, dict) and r.get("success")),
                    "failed_phases": sum(1 for r in results.values() if isinstance(r, dict) and not r.get("success"))
                },
                "detailed_results": results,
                "executive_summary": "Comprehensive AI-powered supply chain security analysis completed successfully",
                "next_steps": [
                    "Review AI-generated threat intelligence",
                    "Implement recommended security measures",
                    "Monitor vector-based threat patterns",
                    "Analyze multimodal evidence for additional insights"
                ]
            }
            
            return {"success": True, "data": report}
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _estimate_ai_query_cost(self, query_type: str, processing_time: float) -> float:
        """Estimate cost for AI queries"""
        # Simplified cost estimation
        base_cost = 0.001  # $0.001 per query
        time_multiplier = processing_time / 60.0  # Cost increases with processing time
        
        return base_cost * (1 + time_multiplier)
    
    def _estimate_vector_query_cost(self, query_type: str, processing_time: float) -> float:
        """Estimate cost for vector queries"""
        # Simplified cost estimation for vector operations
        base_cost = 0.0005  # $0.0005 per vector operation
        time_multiplier = processing_time / 60.0
        
        return base_cost * (1 + time_multiplier)
    
    def _analyze_image_asset(self, asset: Any) -> Dict[str, Any]:
        """Analyze image asset using AI"""
        return {
            "success": True,
            "analysis_type": "image",
            "insights": "Image analysis completed using AI vision models",
            "risk_indicators": ["visual anomalies detected", "suspicious patterns identified"],
            "confidence_score": 0.85
        }
    
    def _analyze_document_asset(self, asset: Any) -> Dict[str, Any]:
        """Analyze document asset using AI"""
        return {
            "success": True,
            "analysis_type": "document",
            "insights": "Document analysis completed using AI text models",
            "risk_indicators": ["suspicious content detected", "malicious patterns identified"],
            "confidence_score": 0.90
        }
    
    def _analyze_video_asset(self, asset: Any) -> Dict[str, Any]:
        """Analyze video asset using AI"""
        return {
            "success": True,
            "analysis_type": "video",
            "insights": "Video analysis completed using AI video models",
            "risk_indicators": ["suspicious activity detected", "anomalous behavior identified"],
            "confidence_score": 0.80
        }
    
    def _analyze_generic_asset(self, asset: Any) -> Dict[str, Any]:
        """Analyze generic asset using AI"""
        return {
            "success": True,
            "analysis_type": "generic",
            "insights": "Generic asset analysis completed using AI models",
            "risk_indicators": ["general risk assessment completed"],
            "confidence_score": 0.75
        }
    
    def _update_analysis_timestamp(self, asset_id: str):
        """Update last analyzed timestamp for asset"""
        try:
            update_query = f"""
            UPDATE `{config.gcp_project_id}.{config.gcp_dataset_id}.supply_chain_assets`
            SET last_analyzed = CURRENT_TIMESTAMP()
            WHERE asset_id = '{asset_id}'
            """
            
            self.client.query(update_query)
            
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Failed to update analysis timestamp: {str(e)}")
    
    def create_demo_threat_reports(self):
        """Create demo threat reports table"""
        try:
            table_id = f"{config.gcp_project_id}.{config.gcp_dataset_id}.demo_threat_reports"
            
            schema = [
                bigquery.SchemaField("report_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("vendor_name", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("threat_type", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("severity", "INTEGER", mode="REQUIRED"),
                bigquery.SchemaField("description", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("raw_report", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED"),
                bigquery.SchemaField("embedding", "FLOAT64", mode="REPEATED")
            ]
            
            table = bigquery.Table(table_id, schema=schema)
            
            try:
                self.client.get_table(table)
                console.print(f"‚úÖ Demo threat reports table already exists")
            except Exception:
                self.client.create_table(table)
                console.print(f"‚úÖ Created demo threat reports table")
                
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Warning: Could not create demo threat reports table: {e}")
    
    def create_infrastructure_table(self):
        """Create infrastructure object table"""
        try:
            table_id = f"{config.gcp_project_id}.{config.gcp_dataset_id}.infrastructure_objects"
            
            schema = [
                bigquery.SchemaField("object_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("object_type", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("object_data", "OBJECT", mode="NULLABLE"),
                bigquery.SchemaField("metadata", "STRING", mode="NULLABLE"),
                bigquery.SchemaField("created_at", "TIMESTAMP", mode="REQUIRED")
            ]
            
            table = bigquery.Table(table_id, schema=schema)
            
            try:
                self.client.get_table(table)
                console.print(f"‚úÖ Infrastructure objects table already exists")
            except Exception:
                self.client.create_table(table)
                console.print(f"‚úÖ Created infrastructure objects table")
                
        except Exception as e:
            console.print(f"‚ö†Ô∏è  Warning: Could not create infrastructure table: {e}")

# Global unified processor instance
unified_ai_processor = UnifiedAIProcessor()

def main():
    """Main entry point for the unified AI processor"""
    parser = argparse.ArgumentParser(description="Unified AI Processor for Supply Chain Security")
    parser.add_argument("--threat-id", help="Threat report ID for analysis")
    parser.add_argument("--query", help="Text query for vector analysis")
    parser.add_argument("--assets", nargs="+", help="Asset IDs for multimodal analysis")
    parser.add_argument("--demo", action="store_true", help="Run comprehensive demo")
    
    args = parser.parse_args()
    
    processor = unified_ai_processor
    
    if args.demo:
        processor.run_demo()
    elif args.threat_id or args.query or args.assets:
        results = processor.run_comprehensive_supply_chain_analysis(
            threat_report_id=args.threat_id,
            query_text=args.query,
            asset_ids=args.assets
        )
        
        if results.get("success"):
            processor.display_comprehensive_results(results)
        else:
            console.print(f"‚ùå Analysis failed: {results.get('error')}")
    else:
        console.print("Please provide analysis parameters or use --demo for demonstration")

if __name__ == "__main__":
    main()


